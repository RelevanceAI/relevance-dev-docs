---
title: "Getting started with FastAPI + Relevance AI"
---

## Deploying with FastAPI

LLMbda is a play on word with lambda that allows you to connect any fastapi server to your library of steps for chaining.
1. Install:

```python Python 
pip install llmbda_fastapi
```

2. Set your Relevance API key and region

```
SET RELEVANCE_API_KEY=xxx
SET RELEVANCE_REGION=xxx
SET RELEVANCE_PROJECt=xxx
```

or

```
export RELEVANCE_API_KEY=xxx
export RELEVANCE_REGION=xxx
export RELEVNACE_PROJECT=xxx
```

3. Include these 2 lines of code:

```python Python
PUBLIC_URL = "https://where_my_api_is_hosted.com/"

from fastapi import FastAPI
app = FastAPI()

from llmbda_fastapi import create_transformations
create_transformations(app.routes, PUBLIC_URL)
```

If you are working off a local computer you can use ngrok to create a public url:

```
pip install pyngrok
```

```python Python
from fastapi import FastAPI
app = FastAPI()

#add this for ngrok
from pyngrok import ngrok
PUBLIC_URL = ngrok.connect(8000).public_url

#add this
from llmbda_fastapi import create_transformations
create_transformations(app.routes, PUBLIC_URL)
```
Add nest_asyncio if you are in a notebook environment.

```python Python
import nest_asyncio
nest_asyncio.apply()
```

4. Add these options to your existing api endpoints, for example this is a endpoint to "Run GPT2"
```
pip install transformers
```
Full code:

```python Python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi import APIRouter, Query
from pydantic import BaseModel
from llmbda_fastapi.frontend import input_components
app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=['*'],
    allow_credentials=True,
    allow_methods=['*'],
    allow_headers=['*'],
)

from transformers import pipeline, set_seed
gpt2_model = pipeline('text-generation', model='gpt2')
set_seed(42)

from llmbda_fastapi.frontend import upload
from typing import List


class GPT2Params(BaseModel):
    prompt : str = Query(..., description="Prompt", frontend=input_components.LongText())

class GPT2ResponseParams(BaseModel):
    results : List[str] = Query(" ", description="Return GPT2 generated text completion")

def transform_func(prompt):
    return {"results" : [t['generated_text'] for t in gpt2_model(prompt)]}

@app.post("/gpt2", name="Run GPT2", description="Run GPT2", response_model=GPT2ResponseParams)
def gpt2_api(commons: GPT2Params):
    return transform_func(commons.prompt)

#add this for ngrok
from pyngrok import ngrok
PUBLIC_URL = ngrok.connect(8000).public_url
print('Public URL:', PUBLIC_URL)

#add this for notebook
#import nest_asyncio
#nest_asyncio.apply()

#add this
from llmbda_fastapi import create_chains
create_chains(app.routes, PUBLIC_URL)

import uvicorn
uvicorn.run(app, port=8000)
```